{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:17] âœ… config loaded\n",
      "[17:16:17] LLM=http://61.169.215.178:10000/v1 model=LLM\n",
      "[17:16:17] EMB=http://61.169.215.178:10004 model=EMBEDDING truncate=1536\n",
      "[17:16:17] RERANK=http://61.169.215.178:10005 model=RERANKER\n",
      "[17:16:17] PG=61.169.215.186:15432/vectordb\n"
     ]
    }
   ],
   "source": [
    "# ==================== æœ€å°å®ç°ï¼šç»Ÿä¸€é…ç½®ï¼ˆåªæ”¹è¿™é‡Œï¼‰ ====================\n",
    "\n",
    "# ---- æœåŠ¡é…ç½®ï¼ˆOpenAI-compatibleï¼‰----\n",
    "HOST_IP = \"61.169.215.178\"\n",
    "LLM_PORT = 10000\n",
    "EMBEDDING_PORT = 10004\n",
    "RERANKER_PORT = 10005\n",
    "API_KEY = \"xtsk\"\n",
    "\n",
    "LLM_MODEL = \"LLM\"\n",
    "EMBEDDING_MODEL = \"EMBEDDING\"\n",
    "RERANKER_MODEL = \"RERANKER\"\n",
    "\n",
    "# ---- pgvector é…ç½® ----\n",
    "PG_HOST = \"61.169.215.186\"\n",
    "PG_PORT = 15432\n",
    "PG_DB = \"vectordb\"\n",
    "PG_USER = \"admin\"\n",
    "PG_PASSWORD = \"xtsk@2024\"\n",
    "\n",
    "# ---- å‘é‡ç»´åº¦ï¼ˆQwen3 Matryoshka æˆªæ–­ï¼‰----\n",
    "TARGET_DIM = 1536  # pgvector index é™åˆ¶ 2000 ç»´\n",
    "\n",
    "# ---- è¿è¡Œå‚æ•° ----\n",
    "INGEST_N_CASES = 5269\n",
    "MAX_WORKERS = 5\n",
    "LLM_TIMEOUT_S = 80\n",
    "EMBED_TIMEOUT_S = 60\n",
    "RERANK_TIMEOUT_S = 30\n",
    "\n",
    "# ---- imports / clients ----\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, TypedDict, Annotated, Sequence\n",
    "\n",
    "import requests\n",
    "import psycopg2\n",
    "from openai import OpenAI\n",
    "\n",
    "EMBEDDING_URL = f\"http://{HOST_IP}:{EMBEDDING_PORT}\"\n",
    "RERANKER_URL = f\"http://{HOST_IP}:{RERANKER_PORT}\"\n",
    "LLM_BASE_URL = f\"http://{HOST_IP}:{LLM_PORT}/v1\"\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": PG_HOST,\n",
    "    \"port\": PG_PORT,\n",
    "    \"database\": PG_DB,\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "}\n",
    "\n",
    "llm_client = OpenAI(api_key=API_KEY, base_url=LLM_BASE_URL)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "\n",
    "log(\"âœ… config loaded\")\n",
    "log(f\"LLM={LLM_BASE_URL} model={LLM_MODEL} timeout={LLM_TIMEOUT_S}s\")\n",
    "log(f\"EMB={EMBEDDING_URL} model={EMBEDDING_MODEL} truncate={TARGET_DIM} timeout={EMBED_TIMEOUT_S}s\")\n",
    "log(f\"RERANK={RERANKER_URL} model={RERANKER_MODEL} timeout={RERANK_TIMEOUT_S}s\")\n",
    "log(f\"PG={PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "log(f\"INGEST_N_CASES={INGEST_N_CASES} MAX_WORKERS={MAX_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LLM ç»“æ„åŒ–æŠ½å– ====================\n",
    "\n",
    "with open(\"corpus/_all_cases.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_cases = json.load(f)\n",
    "\n",
    "STRUCTURE_PROMPT = \"\"\"ä½ æ˜¯ä¸­åŒ»åŒ»æ¡ˆç»“æ„åŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹åŒ»æ¡ˆå†…å®¹æå–ä¸ºæ ‡å‡† JSONã€‚\n",
    "\n",
    "åªè¾“å‡º JSONï¼š\n",
    "{\n",
    "  \\\"patient\\\": {\\\"age\\\": int|null, \\\"sex\\\": \\\"ç”·\\\"|\\\"å¥³\\\"|null},\n",
    "  \\\"chief_complaint\\\": str|null,\n",
    "  \\\"tcm_diagnosis\\\": str|null,\n",
    "  \\\"western_diagnosis\\\": str|null,\n",
    "  \\\"tcm_pattern\\\": str|null,\n",
    "  \\\"treatment_principle\\\": str|null,\n",
    "  \\\"prescriptions\\\": [{\\\"visit\\\": str, \\\"herbs\\\": [{\\\"name\\\": str, \\\"dose\\\": str}], \\\"days\\\": int|null}],\n",
    "  \\\"outcome\\\": str|null,\n",
    "  \\\"summary\\\": str\n",
    "}\n",
    "\n",
    "å­—æ®µä¸ºç©ºç”¨ nullã€‚\n",
    "\n",
    "åŒ»æ¡ˆå†…å®¹ï¼š\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def strip_code_fence(text: str) -> str:\n",
    "    t = (text or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        parts = t.split(\"```\")\n",
    "        if len(parts) >= 2:\n",
    "            t = parts[1].strip()\n",
    "            if t.startswith(\"json\"):\n",
    "                t = t[4:].strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def extract_case_structure(content: str, max_retries: int = 3) -> Optional[Dict]:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = llm_client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"åªè¾“å‡º JSONã€‚\"},\n",
    "                    {\"role\": \"user\", \"content\": STRUCTURE_PROMPT + content[:5000]},\n",
    "                ],\n",
    "                temperature=0,\n",
    "                timeout=LLM_TIMEOUT_S,\n",
    "            )\n",
    "            return json.loads(strip_code_fence(resp.choices[0].message.content))\n",
    "        except Exception as e:\n",
    "            log(f\"LLM extract failed attempt={attempt}/{max_retries}: {str(e)[:120]}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(2)\n",
    "    return None\n",
    "\n",
    "\n",
    "log(f\"cases loaded: {all_cases.get('count')} total\")\n",
    "log(f\"preview: {all_cases['cases'][0]['content'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f038db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== å»ºè¡¨ + å…¥åº“ï¼ˆå‰ N æ¡ï¼‰ ====================\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "def init_tables():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS tcm_case_embedding CASCADE;\")\n",
    "    cur.execute(\"DROP TABLE IF EXISTS tcm_case CASCADE;\")\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE tcm_case (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            source_md TEXT,\n",
    "            case_index INT,\n",
    "            raw_content TEXT,\n",
    "            clean_json JSONB,\n",
    "            sex TEXT,\n",
    "            age INT,\n",
    "            tcm_pattern TEXT,\n",
    "            tcm_diagnosis TEXT,\n",
    "            western_diagnosis TEXT,\n",
    "            summary TEXT,\n",
    "            created_at TIMESTAMP DEFAULT NOW()\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE tcm_case_embedding (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            case_id INT REFERENCES tcm_case(id) ON DELETE CASCADE,\n",
    "            view TEXT,\n",
    "            content TEXT,\n",
    "            embedding vector({TARGET_DIM}),\n",
    "            created_at TIMESTAMP DEFAULT NOW()\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\"CREATE INDEX idx_embedding_view ON tcm_case_embedding(view);\")\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE INDEX idx_embedding_vector\n",
    "        ON tcm_case_embedding\n",
    "        USING hnsw (embedding vector_cosine_ops);\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def embed_truncated(text: str, max_retries: int = 3) -> Optional[List[float]]:\n",
    "    url = f\"{EMBEDDING_URL}/v1/embeddings\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    payload = {\"model\": EMBEDDING_MODEL, \"input\": [text]}\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.post(url, headers=headers, json=payload, timeout=EMBED_TIMEOUT_S)\n",
    "            r.raise_for_status()\n",
    "            full = r.json()[\"data\"][0][\"embedding\"]\n",
    "            return full[:TARGET_DIM]\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                log(f\"embedding failed: {str(e)[:120]}\")\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def process_one_case(case_data: Dict, idx: int, total: int) -> bool:\n",
    "    log(f\"[{idx}/{total}] start {case_data['source_md']}#{case_data['index']}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    structured = extract_case_structure(case_data[\"content\"], max_retries=3)\n",
    "    t_llm = time.time() - t0\n",
    "\n",
    "    if not structured:\n",
    "        log(f\"[{idx}/{total}] LLM failed ({t_llm:.1f}s)\")\n",
    "        return False\n",
    "\n",
    "    log(f\"[{idx}/{total}] LLM ok ({t_llm:.1f}s)\")\n",
    "\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO tcm_case (source_md, case_index, raw_content, clean_json, sex, age, tcm_pattern, tcm_diagnosis, western_diagnosis, summary)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        RETURNING id;\n",
    "        \"\"\",\n",
    "        (\n",
    "            case_data[\"source_md\"],\n",
    "            case_data[\"index\"],\n",
    "            case_data[\"content\"],\n",
    "            json.dumps(structured, ensure_ascii=False),\n",
    "            (structured.get(\"patient\") or {}).get(\"sex\"),\n",
    "            (structured.get(\"patient\") or {}).get(\"age\"),\n",
    "            structured.get(\"tcm_pattern\"),\n",
    "            structured.get(\"tcm_diagnosis\"),\n",
    "            structured.get(\"western_diagnosis\"),\n",
    "            structured.get(\"summary\"),\n",
    "        ),\n",
    "    )\n",
    "    case_id = cur.fetchone()[0]\n",
    "\n",
    "    views = [\n",
    "        (\"summary\", structured.get(\"summary\") or \"\"),\n",
    "        (\"pattern\", f\"{structured.get('tcm_pattern') or ''} {structured.get('treatment_principle') or ''}\".strip()),\n",
    "    ]\n",
    "\n",
    "    for view, content in views:\n",
    "        if not content:\n",
    "            continue\n",
    "        t1 = time.time()\n",
    "        emb = embed_truncated(content[:500], max_retries=3)\n",
    "        t_emb = time.time() - t1\n",
    "        if emb is None:\n",
    "            log(f\"[{idx}/{total}] embedding failed view={view} ({t_emb:.1f}s)\")\n",
    "            continue\n",
    "        cur.execute(\n",
    "            \"\"\"INSERT INTO tcm_case_embedding (case_id, view, content, embedding) VALUES (%s,%s,%s,%s);\"\"\",\n",
    "            (case_id, view, content, emb),\n",
    "        )\n",
    "        log(f\"[{idx}/{total}] embedding ok view={view} ({t_emb:.1f}s)\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    log(f\"[{idx}/{total}] done case_id={case_id}\\n\")\n",
    "    return True\n",
    "\n",
    "\n",
    "init_tables()\n",
    "\n",
    "cases_to_ingest = all_cases[\"cases\"][:INGEST_N_CASES]\n",
    "log(f\"ingest N={len(cases_to_ingest)} workers={MAX_WORKERS}\")\n",
    "\n",
    "ok = 0\n",
    "fail = 0\n",
    "lock = Lock()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = [ex.submit(process_one_case, c, i + 1, len(cases_to_ingest)) for i, c in enumerate(cases_to_ingest)]\n",
    "    for fu in as_completed(futures):\n",
    "        try:\n",
    "            success = fu.result()\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            log(f\"task exception: {str(e)[:120]}\")\n",
    "\n",
    "        with lock:\n",
    "            ok += 1 if success else 0\n",
    "            fail += 0 if success else 1\n",
    "\n",
    "log(f\"ingest done success={ok} fail={fail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79019ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RAG æ£€ç´¢ï¼ˆmetadata + vector + rerankï¼‰ ====================\n",
    "\n",
    "class TCMCaseRAG:\n",
    "    def _query_embedding(self, text: str) -> List[float]:\n",
    "        url = f\"{EMBEDDING_URL}/v1/embeddings\"\n",
    "        headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "        payload = {\"model\": EMBEDDING_MODEL, \"input\": [text]}\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=EMBED_TIMEOUT_S)\n",
    "        r.raise_for_status()\n",
    "        full = r.json()[\"data\"][0][\"embedding\"]\n",
    "        return full[:TARGET_DIM]\n",
    "\n",
    "    def _rerank(self, query: str, candidates: List[Dict], top_k: int) -> List[Dict]:\n",
    "        url = f\"{RERANKER_URL}/v1/rerank\"\n",
    "        headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "        docs = [c.get(\"summary\") or c.get(\"content\", \"\")[:300] for c in candidates]\n",
    "        payload = {\"model\": RERANKER_MODEL, \"query\": query, \"documents\": docs, \"top_k\": min(top_k, len(docs))}\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=RERANK_TIMEOUT_S)\n",
    "        r.raise_for_status()\n",
    "        out = []\n",
    "        for item in r.json()[\"results\"]:\n",
    "            idx = item[\"index\"]\n",
    "            c = candidates[idx]\n",
    "            c[\"rerank_score\"] = item[\"relevance_score\"]\n",
    "            out.append(c)\n",
    "        return out\n",
    "\n",
    "    def search(self, query: str, filters: Optional[Dict] = None, top_k: int = 10, rerank_top_k: int = 5, view: str = \"summary\") -> List[Dict]:\n",
    "        qv = self._query_embedding(query)\n",
    "\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        sql = \"\"\"\n",
    "            SELECT\n",
    "                c.id, c.source_md, c.case_index, c.summary, c.sex, c.age, c.tcm_pattern,\n",
    "                c.tcm_diagnosis, c.western_diagnosis, c.clean_json, e.content,\n",
    "                1 - (e.embedding <=> %s::vector) AS similarity\n",
    "            FROM tcm_case c\n",
    "            JOIN tcm_case_embedding e ON c.id = e.case_id\n",
    "            WHERE e.view = %s\n",
    "        \"\"\"\n",
    "        params = [qv, view]\n",
    "\n",
    "        if filters:\n",
    "            if filters.get(\"sex\"):\n",
    "                sql += \" AND c.sex = %s\"\n",
    "                params.append(filters[\"sex\"])\n",
    "            if filters.get(\"age_min\") is not None:\n",
    "                sql += \" AND c.age >= %s\"\n",
    "                params.append(filters[\"age_min\"])\n",
    "            if filters.get(\"age_max\") is not None:\n",
    "                sql += \" AND c.age <= %s\"\n",
    "                params.append(filters[\"age_max\"])\n",
    "            if filters.get(\"tcm_pattern\"):\n",
    "                sql += \" AND c.tcm_pattern ILIKE %s\"\n",
    "                params.append(f\"%{filters['tcm_pattern']}%\")\n",
    "\n",
    "        sql += \" ORDER BY similarity DESC LIMIT %s;\"\n",
    "        params.append(top_k)\n",
    "\n",
    "        cur.execute(sql, params)\n",
    "        rows = cur.fetchall()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        candidates = []\n",
    "        for r in rows:\n",
    "            candidates.append(\n",
    "                {\n",
    "                    \"case_id\": r[0],\n",
    "                    \"source_md\": r[1],\n",
    "                    \"case_index\": r[2],\n",
    "                    \"summary\": r[3],\n",
    "                    \"sex\": r[4],\n",
    "                    \"age\": r[5],\n",
    "                    \"tcm_pattern\": r[6],\n",
    "                    \"tcm_diagnosis\": r[7],\n",
    "                    \"western_diagnosis\": r[8],\n",
    "                    \"clean_json\": r[9],\n",
    "                    \"content\": r[10],\n",
    "                    \"similarity\": float(r[11]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        return self._rerank(query, candidates, rerank_top_k)\n",
    "\n",
    "\n",
    "rag = TCMCaseRAG()\n",
    "\n",
    "q = \"å¤±çœ å¤šæ¢¦ï¼Œå¿ƒçƒ¦æ˜“æ€’ï¼Œå¦‚ä½•æ²»ç–—ï¼Ÿ\"\n",
    "res = rag.search(q, top_k=5, rerank_top_k=3)\n",
    "print(\"RAG test:\", q)\n",
    "for i, r in enumerate(res, 1):\n",
    "    print(i, r[\"source_md\"], r[\"case_index\"], f\"sim={r['similarity']:.3f}\", f\"rerank={r.get('rerank_score', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df20385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æµ‹è¯• RAG æ£€ç´¢\n",
      "\n",
      "æŸ¥è¯¢: å¤±çœ å¤šæ¢¦ï¼Œå¿ƒçƒ¦æ˜“æ€’ï¼Œå¦‚ä½•æ²»ç–—ï¼Ÿ\n",
      "å¬å› 5 ä¸ªåŒ»æ¡ˆ:\n",
      "\n",
      "1. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#7] ç›¸ä¼¼åº¦:0.776 Rerank:0.884\n",
      "   æ‚£è€…: å¥³/31å² | è¯å‹: è‚äº¢è‚¾è™š\n",
      "   æ‘˜è¦: ä¸»è¦ç—‡çŠ¶ä¸ºé•¿æœŸå¤±çœ ã€å¤šæ¢¦ã€å¤œå°¿é¢‘æ€¥ã€å¤´èƒ€ç—›ã€é¢ˆéƒ¨æ¿æ»ã€æ‰‹éº»ã€å¿ƒçƒ¦ç´§å¼ ã€è…°é…¸è…¿éº»ï¼›è¯å‹ä¸ºè‚äº¢è‚¾è™šï¼›æ ¸å¿ƒæ²»æ³•ä¸ºå¹³è‚ç›Šè‚¾å®‰ç¥ï¼›ç»åŠ å‘³é¾™ç‰¡æ±¤åˆä»™åœ°æ±¤åŠ å‡æ²»ç–—ï¼ŒäºŒè¯Šåç¡çœ æ˜æ˜¾æ”¹å–„ï¼Œè¯¸ç—‡å‡è½»ï¼Œç–—æ•ˆæ˜¾è‘—ã€‚...\n",
      "\n",
      "2. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#9] ç›¸ä¼¼åº¦:0.715 Rerank:0.875\n",
      "   æ‚£è€…: å¥³/42å² | è¯å‹: è‚éƒé˜³äº¢ï¼Œæ°”è¡€ç—¹é˜»\n",
      "   æ‘˜è¦: æ‚£è€…ä¸»è¦ç—‡çŠ¶ä¸ºé•¿æœŸå¤±çœ ä¼´å…¥ç¡å›°éš¾ã€å¤šæ¢¦ã€å¤´æ™•å¤´èƒ€ã€å£å¹²ã€å¿ƒæ…Œã€ä¾¿ç§˜ç­‰ï¼Œè¯å±è‚éƒé˜³äº¢ã€æ°”è¡€ç—¹é˜»ã€‚æ ¸å¿ƒæ²»æ³•ä¸ºè§£éƒå¹³è‚ã€é€šç»œæ´»è¡€ã€æ¸…çƒ­å®‰ç¥ï¼Œæ–¹ç”¨ç”˜éº¦è‹¦å‚æ±¤åˆåŠ å‘³æŸ´èƒ¡é¾™ç‰¡æ±¤åŠ å‡ã€‚ç»æ²»ç–—åç¡çœ æ˜æ˜¾æ”¹å–„ï¼Œè¯¸ç—‡å‡è½»...\n",
      "\n",
      "3. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#8] ç›¸ä¼¼åº¦:0.732 Rerank:0.874\n",
      "   æ‚£è€…: å¥³/24å² | è¯å‹: è‚æœ¨åæ—ºï¼Œç˜€çƒ­äº¤é˜»\n",
      "   æ‘˜è¦: ä¸»è¦ç—‡çŠ¶ä¸ºå¤±çœ ã€å¤šæ¢¦æ˜“é†’ã€å¿ƒçƒ¦æ˜“æ€’ã€é¢ˆæ¿ä¸é€‚ï¼›è¯å‹ä¸ºè‚æœ¨åæ—ºï¼Œç˜€çƒ­äº¤é˜»ï¼›æ ¸å¿ƒæ²»æ³•ä¸ºå¹³è‚æŠ‘æœ¨ï¼Œæ¸…çƒ­åŒ–ç˜€ï¼›ç»ä¸‰è¯Šæ²»ç–—åå¤œå¯æ¢å¤æ­£å¸¸ï¼Œç–—æ•ˆæ˜¾è‘—ã€‚...\n",
      "\n",
      "4. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#15] ç›¸ä¼¼åº¦:0.745 Rerank:0.872\n",
      "   æ‚£è€…: ç”·/69å² | è¯å‹: è‚é˜³ä¸Šäº¢åŒ–é£ï¼Œç˜€çƒ­äº¤é˜»\n",
      "   æ‘˜è¦: ä¸»è¦ç—‡çŠ¶ä¸ºå¤œå¯å¤šæ¢¦ã€å™©æ¢¦ã€è¯´æ¢¦è¯ã€æ¢¦ä¸­ç´§å¼ åŠè¸¢æ‰“åŠ¨ä½œï¼›è¯å‹ä¸ºè‚é˜³ä¸Šäº¢åŒ–é£ï¼Œç˜€çƒ­äº¤é˜»ï¼›æ ¸å¿ƒæ²»æ³•ä¸ºå¹³è‚æ½œé˜³æ¯é£ï¼Œæ¸…çƒ­æ´»è¡€å®‰ç¥ï¼›ç»28å‰‚ä¸­è¯æ²»ç–—åç—‡çŠ¶æ˜æ˜¾å‡è½»ï¼Œç–—æ•ˆæ˜¾è‘—ã€‚...\n",
      "\n",
      "5. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#10] ç›¸ä¼¼åº¦:0.723 Rerank:0.871\n",
      "   æ‚£è€…: å¥³/50å² | è¯å‹: è‚äº¢è‚¾è™š\n",
      "   æ‘˜è¦: ä¸»è¦ç—‡çŠ¶ä¸ºå¤±çœ ã€å¤šæ¢¦ã€æ—©é†’ã€çƒ˜çƒ­æ±—å‡ºã€è…°é…¸ç—›ã€å°¿é¢‘ç­‰ï¼›è¯å‹ä¸ºè‚äº¢è‚¾è™šï¼›æ ¸å¿ƒæ²»æ³•ä¸ºç›Šè‚¾å¹³è‚ã€è§£éƒå®‰ç¥ï¼›ç–—æ•ˆæ˜¾è‘—ï¼Œç¡çœ æ˜æ˜¾æ”¹å–„ï¼Œä¼´éšç—‡çŠ¶ç¼“è§£ã€‚...\n",
      "\n",
      "\n",
      "æŸ¥è¯¢: å¥³æ€§æ›´å¹´æœŸå¤±çœ æ€ä¹ˆåŠï¼Ÿ\n",
      "è¿‡æ»¤: {'sex': 'å¥³', 'age_min': 40, 'age_max': 55}\n",
      "å¬å› 5 ä¸ªåŒ»æ¡ˆ:\n",
      "\n",
      "1. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#20] å¥³/46å² | è‚äº¢è‚¾è™š\n",
      "   Rerankåˆ†æ•°: 0.833\n",
      "2. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#10] å¥³/50å² | è‚äº¢è‚¾è™š\n",
      "   Rerankåˆ†æ•°: 0.832\n",
      "3. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#5] å¥³/47å² | è‚äº¢è‚¾è™šï¼Œæ¹¿çƒ­ä¸‹æ³¨\n",
      "   Rerankåˆ†æ•°: 0.825\n",
      "4. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#9] å¥³/42å² | è‚éƒé˜³äº¢ï¼Œæ°”è¡€ç—¹é˜»\n",
      "   Rerankåˆ†æ•°: 0.822\n",
      "5. [ä¸Šæµ·åè€ä¸­åŒ»åŒ»æ¡ˆç²¾é€‰.md#3] å¥³/48å² | è‚èƒ†å¤±ç–ï¼Œæ¹¿çƒ­è•´ç»“\n",
      "   Rerankåˆ†æ•°: 0.820\n"
     ]
    }
   ],
   "source": [
    "# ==================== LangGraph Agentï¼ˆparse â†’ retrieve â†’ generateï¼‰ ====================\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    parsed_query: dict\n",
    "    retrieved_cases: list\n",
    "    answer: str\n",
    "    messages: Annotated[Sequence[str], operator.add]\n",
    "\n",
    "\n",
    "def parse_query_node(state: AgentState) -> AgentState:\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    prompt = f\"\"\"è¯·æŠŠç”¨æˆ·é—®é¢˜è§£ææˆæ£€ç´¢ç”¨ JSONã€‚åªè¾“å‡º JSONï¼š\n",
    "{{\n",
    "  \\\"filters\\\": {{\\\"sex\\\": \\\"ç”·\\\"|\\\"å¥³\\\"|null, \\\"age_min\\\": int|null, \\\"age_max\\\": int|null, \\\"tcm_pattern\\\": str|null}},\n",
    "  \\\"rewritten_query\\\": str\n",
    "}}\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜: {query}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"åªè¾“å‡º JSONã€‚\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            timeout=LLM_TIMEOUT_S,\n",
    "        )\n",
    "        parsed = json.loads(strip_code_fence(resp.choices[0].message.content))\n",
    "        state[\"parsed_query\"] = parsed\n",
    "        state[\"messages\"].append(f\"parse: {parsed}\")\n",
    "    except Exception as e:\n",
    "        state[\"parsed_query\"] = {\"filters\": {}, \"rewritten_query\": query}\n",
    "        state[\"messages\"].append(f\"parse_failed: {str(e)[:80]}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def retrieve_cases_node(state: AgentState) -> AgentState:\n",
    "    parsed = state.get(\"parsed_query\") or {}\n",
    "    filters = {k: v for k, v in (parsed.get(\"filters\") or {}).items() if v is not None}\n",
    "    rewritten = parsed.get(\"rewritten_query\") or state[\"query\"]\n",
    "\n",
    "    try:\n",
    "        results = rag.search(rewritten, filters=filters or None, top_k=8, rerank_top_k=5)\n",
    "        state[\"retrieved_cases\"] = results\n",
    "        state[\"messages\"].append(f\"retrieve: {len(results)} cases\")\n",
    "    except Exception as e:\n",
    "        state[\"retrieved_cases\"] = []\n",
    "        state[\"messages\"].append(f\"retrieve_failed: {str(e)[:80]}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_answer_node(state: AgentState) -> AgentState:\n",
    "    query = state[\"query\"]\n",
    "    cases = state.get(\"retrieved_cases\") or []\n",
    "\n",
    "    if not cases:\n",
    "        state[\"answer\"] = \"æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³åŒ»æ¡ˆã€‚\"\n",
    "        state[\"messages\"].append(\"generate: no cases\")\n",
    "        return state\n",
    "\n",
    "    ctx = \"\"\n",
    "    for i, c in enumerate(cases[:3], 1):\n",
    "        ctx += f\"\\n[åŒ»æ¡ˆ{i}] {c['source_md']}#{c['case_index']} | {c.get('sex')}/{c.get('age')} | {c.get('tcm_pattern')}\\n\"\n",
    "        ctx += f\"æ‘˜è¦: {c.get('summary','')}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸­åŒ»ä¸“å®¶ã€‚åŸºäºæ£€ç´¢åˆ°çš„åŒ»æ¡ˆå›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜: {query}\n",
    "\n",
    "æ£€ç´¢åˆ°çš„åŒ»æ¡ˆæ‘˜è¦:\n",
    "{ctx}\n",
    "\n",
    "è¦æ±‚:\n",
    "- ç»“åˆåŒ»æ¡ˆç»™å‡ºè¾¨è¯æ€è·¯ã€æ²»æ³•å’Œç”¨è¯æ–¹å‘\n",
    "- å¼•ç”¨åŒ»æ¡ˆç¼–å·\n",
    "- 300å­—å†…\n",
    "\n",
    "å›ç­”:\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸­åŒ»ä¸“å®¶ã€‚\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            timeout=LLM_TIMEOUT_S,\n",
    "        )\n",
    "        state[\"answer\"] = resp.choices[0].message.content.strip()\n",
    "        state[\"messages\"].append(\"generate: ok\")\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"ç”Ÿæˆå¤±è´¥: {str(e)[:120]}\"\n",
    "        state[\"messages\"].append(\"generate_failed\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def build_agent():\n",
    "    g = StateGraph(AgentState)\n",
    "    g.add_node(\"parse\", parse_query_node)\n",
    "    g.add_node(\"retrieve\", retrieve_cases_node)\n",
    "    g.add_node(\"generate\", generate_answer_node)\n",
    "\n",
    "    g.set_entry_point(\"parse\")\n",
    "    g.add_edge(\"parse\", \"retrieve\")\n",
    "    g.add_edge(\"retrieve\", \"generate\")\n",
    "    g.add_edge(\"generate\", END)\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "agent = build_agent()\n",
    "print(\"âœ… agent ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ç¤ºä¾‹è°ƒç”¨ ====================\n",
    "\n",
    "def run(query: str):\n",
    "    state = {\"query\": query, \"parsed_query\": {}, \"retrieved_cases\": [], \"answer\": \"\", \"messages\": []}\n",
    "    out = agent.invoke(state)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Q:\", query)\n",
    "    print(\"--- messages ---\")\n",
    "    for m in out[\"messages\"]:\n",
    "        print(m)\n",
    "    print(\"--- answer ---\")\n",
    "    print(out[\"answer\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "run(\"æˆ‘æœ€è¿‘å¤±çœ å¤šæ¢¦ï¼Œå¿ƒçƒ¦æ˜“æ€’ï¼Œä¸­åŒ»æ€ä¹ˆæ²»ç–—ï¼Ÿ\")\n",
    "run(\"50å²å¥³æ€§ï¼Œç»ç»åå¤±çœ ä¸¥é‡ï¼Œçƒ˜çƒ­æ±—å‡ºï¼Œè…°é…¸ï¼Œæ€ä¹ˆåŠï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2f5ec",
   "metadata": {},
   "source": [
    "# AgentState å·¥ä½œåŸç†ï¼ˆé€šä¿—è§£é‡Šï¼‰\n",
    "\n",
    "`AgentState` å¯ä»¥ç†è§£æˆ **Agent åœ¨æµæ°´çº¿é‡Œä¼ é€’çš„ä¸€å¼ â€œå·¥ä½œè¡¨â€**ã€‚æ¯è·‘å®Œä¸€ä¸ªèŠ‚ç‚¹ï¼Œå°±åœ¨è¿™å¼ è¡¨ä¸Šâ€œå¡«å‡ æ ä¿¡æ¯â€ï¼Œäº¤ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ç»§ç»­ç”¨ã€‚\n",
    "\n",
    "å®ƒåŒ…å«è¿™äº›å­—æ®µï¼š\n",
    "\n",
    "- `query`\n",
    "  - ç”¨æˆ·åŸå§‹é—®é¢˜ã€‚\n",
    "  - ä¾‹ï¼š\"50å²å¥³æ€§ç»ç»åå¤±çœ ä¸¥é‡æ€ä¹ˆåŠï¼Ÿ\"\n",
    "\n",
    "- `parsed_query`\n",
    "  - **æŠŠç”¨æˆ·é—®é¢˜ç¿»è¯‘æˆæ£€ç´¢è¯­è¨€**åçš„ç»“æœã€‚\n",
    "  - é‡Œé¢é€šå¸¸æœ‰ï¼š\n",
    "    - `filters`ï¼šç¡¬è¿‡æ»¤æ¡ä»¶ï¼ˆæ€§åˆ«ã€å¹´é¾„æ®µã€è¯å‹å…³é”®è¯ç­‰ï¼‰\n",
    "    - `rewritten_query`ï¼šä¸ºäº†å‘é‡æ£€ç´¢æ›´å‡†ï¼Œå¯¹é—®é¢˜åšçš„â€œæ£€ç´¢æ”¹å†™â€\n",
    "\n",
    "- `retrieved_cases`\n",
    "  - RAG æ£€ç´¢å¾—åˆ°çš„å€™é€‰åŒ»æ¡ˆåˆ—è¡¨ï¼ˆå·²ç»åšè¿‡å‘é‡å¬å› + rerank æ’åºï¼‰ã€‚\n",
    "\n",
    "- `answer`\n",
    "  - æœ€ç»ˆè¦ç»™ç”¨æˆ·çš„å›ç­”æ–‡æœ¬ã€‚\n",
    "\n",
    "- `messages`\n",
    "  - **æµæ°´çº¿æ—¥å¿—**ã€‚æ¯ä¸ªèŠ‚ç‚¹ä¼šå¾€é‡Œé¢è¿½åŠ ä¸€å¥è¯ï¼Œå¸®åŠ©ä½ çœ‹æ¸… agent æ¯ä¸€æ­¥åˆ°åº•åšäº†ä»€ä¹ˆã€‚\n",
    "\n",
    "## 3 ä¸ªèŠ‚ç‚¹åˆ†åˆ«å¹²ä»€ä¹ˆ\n",
    "\n",
    "### 1) `parse` èŠ‚ç‚¹ï¼ˆparse_query_nodeï¼‰\n",
    "æŠŠè‡ªç„¶è¯­è¨€é—®é¢˜å˜æˆâ€œæ£€ç´¢å‚æ•°â€ã€‚\n",
    "\n",
    "- è¾“å…¥ï¼š`query`\n",
    "- è¾“å‡ºï¼šå†™å…¥ `parsed_query`\n",
    "\n",
    "é€šä¿—ç†è§£ï¼š\n",
    "- ç”¨æˆ·è¯´â€œ50å²å¥³æ€§ç»ç»åå¤±çœ ã€çƒ˜çƒ­æ±—å‡ºâ€\n",
    "- è¿™ä¸ªèŠ‚ç‚¹ä¼šæå–ï¼š\n",
    "  - è¿‡æ»¤æ¡ä»¶ï¼š`sex=å¥³`ï¼Œ`age_min=45`ï¼Œ`age_max=55`ï¼ˆå¦‚æœèƒ½æ¨æ–­ï¼‰\n",
    "  - æ”¹å†™æŸ¥è¯¢ï¼šæŠŠå£è¯­å˜æˆæ›´åƒâ€œå…³é”®è¯+ç—‡çŠ¶+è¯å€™â€çš„æ£€ç´¢å¥å­\n",
    "\n",
    "### 2) `retrieve` èŠ‚ç‚¹ï¼ˆretrieve_cases_nodeï¼‰\n",
    "çœŸæ­£å»æ•°æ®åº“é‡Œâ€œæ‰¾åŒ»æ¡ˆâ€ã€‚\n",
    "\n",
    "- è¾“å…¥ï¼š`parsed_query`\n",
    "- æ‰§è¡Œï¼š\n",
    "  - å…ˆç”¨ `rewritten_query` ç”Ÿæˆ embeddingï¼ˆå¹¶æˆªæ–­åˆ° `TARGET_DIM`ï¼Œä¸å…¥åº“ä¸€è‡´ï¼‰\n",
    "  - pgvector åšå‘é‡ç›¸ä¼¼åº¦å¬å›ï¼ˆå¯å åŠ  filtersï¼‰\n",
    "  - reranker å†å¯¹å¬å›ç»“æœé‡æ’\n",
    "- è¾“å‡ºï¼šå†™å…¥ `retrieved_cases`\n",
    "\n",
    "é€šä¿—ç†è§£ï¼š\n",
    "- å…ˆâ€œç²—æ‰¾â€ï¼šå‘é‡æ‰¾ç›¸ä¼¼åŒ»æ¡ˆ\n",
    "- å†â€œç²¾æ’â€ï¼šreranker æŠŠæœ€ç›¸å…³çš„æ’å‰é¢\n",
    "\n",
    "### 3) `generate` èŠ‚ç‚¹ï¼ˆgenerate_answer_nodeï¼‰\n",
    "æŠŠæ£€ç´¢åˆ°çš„åŒ»æ¡ˆæ‘˜è¦å½“ä½œâ€œå‚è€ƒèµ„æ–™â€ï¼Œè®© LLM ç”Ÿæˆå›ç­”ã€‚\n",
    "\n",
    "- è¾“å…¥ï¼š`query` + `retrieved_cases`\n",
    "- è¾“å‡ºï¼šå†™å…¥ `answer`\n",
    "\n",
    "é€šä¿—ç†è§£ï¼š\n",
    "- è¿™ä¸ªèŠ‚ç‚¹ä¸å†â€œå‡­ç©ºå›ç­”â€ï¼Œè€Œæ˜¯ï¼š\n",
    "  - å…ˆæŠŠ top3 åŒ»æ¡ˆæ‹¼æˆä¸Šä¸‹æ–‡\n",
    "  - å†è®© LLM ç»“åˆè¿™äº›åŒ»æ¡ˆå»æ€»ç»“ï¼šè¯å‹ã€æ²»æ³•ã€ç”¨è¯æ–¹å‘\n",
    "\n",
    "---\n",
    "\n",
    "å¦‚æœä½ å¸Œæœ› `parse` ä¸è¦â€œè¿‡åº¦è¿‡æ»¤â€ï¼ˆå¯¼è‡´æ£€ç´¢ 0 æ¡ï¼‰ï¼Œæˆ‘å¯ä»¥åœ¨ `parse_query_node` é‡ŒåŠ ä¸€ä¸ªç­–ç•¥ï¼š\n",
    "- **filters åªæŠ½å– sex/ageï¼Œä¸å¼ºè¡Œç»™ tcm_pattern**ï¼›\n",
    "- æˆ–è€… `tcm_pattern` åªæœ‰åœ¨ç”¨æˆ·æ˜ç¡®è¯´äº†æŸè¯å‹æ‰å¡«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1e52d",
   "metadata": {},
   "source": [
    "# è¿è¡Œé¡ºåº\n",
    "\n",
    "1. è¿è¡Œ `ç»Ÿä¸€é…ç½®`\n",
    "2. è¿è¡Œ `LLM ç»“æ„åŒ–æŠ½å–`\n",
    "3. è¿è¡Œ `å»ºè¡¨ + å…¥åº“ï¼ˆå‰ N æ¡ï¼‰`\n",
    "4. è¿è¡Œ `RAG æ£€ç´¢`\n",
    "5. è¿è¡Œ `LangGraph Agent`\n",
    "6. è¿è¡Œ `ç¤ºä¾‹è°ƒç”¨`\n",
    "\n",
    "å¦‚æœä½ é‡åˆ° LLM è¶…æ—¶ï¼š\n",
    "- ä¼˜å…ˆé™ä½ `MAX_WORKERS` åˆ° 1-2\n",
    "- æˆ–æé«˜ `LLM_TIMEOUT_S`\n",
    "- æˆ–æŠŠ `content[:5000]` è°ƒå°ï¼ˆæ¯”å¦‚ 3000ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
